{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from selenium import webdriver\n",
    "from selenium.webdriver.chrome.service import Service\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.support.ui import WebDriverWait\n",
    "from selenium.webdriver.support import expected_conditions as EC\n",
    "from selenium.webdriver.common.action_chains import ActionChains\n",
    "from webdriver_manager.chrome import ChromeDriverManager\n",
    "from bs4 import BeautifulSoup\n",
    "import csv\n",
    "import time\n",
    "import random\n",
    "\n",
    "\n",
    "options = webdriver.ChromeOptions()\n",
    "options.add_argument(\"--no-sandbox\")\n",
    "options.add_argument(\"--disable-dev-shm-usage\")\n",
    "service = Service(ChromeDriverManager().install())\n",
    "driver = webdriver.Chrome(service=service, options=options)\n",
    "\n",
    "\n",
    "with open(\"album_data.csv\", \"w\", newline=\"\", encoding=\"utf-8\") as file:\n",
    "    writer = csv.writer(file)\n",
    "    writer.writerow([\"Album Name\", \"Artist\", \"Critic Score\", \"User Score\"])\n",
    "\n",
    "    # loop through pages \n",
    "    for page_num in range(1, 5):\n",
    "        page_url = f\"https://www.albumoftheyear.org/ratings/user-highest-rated/all/{page_num}/\"\n",
    "        driver.get(page_url)\n",
    "        print(f\"going to page {page_num}: {page_url}\") #delete after finished\n",
    "\n",
    "        \n",
    "        WebDriverWait(driver, 10).until(EC.presence_of_element_located((By.CSS_SELECTOR, \"a[itemprop='url']\")))\n",
    "        time.sleep(random.uniform(2, 4))  # still trying to fool captcha + wait until everything is loaded onto page\n",
    "\n",
    "        \n",
    "        albums = driver.find_elements(By.CSS_SELECTOR, \"a[itemprop='url']\")\n",
    "\n",
    "        for album in albums:\n",
    "            album_url = \"https://www.albumoftheyear.org\" + album.get_attribute(\"href\")\n",
    "            print(f\"Navigating to album page: {album_url}\")\n",
    "\n",
    "            \n",
    "            driver.execute_script(\"arguments[0].scrollIntoView();\", album)\n",
    "            ActionChains(driver).move_to_element(album).pause(random.uniform(1, 2)).click().perform()\n",
    "\n",
    "            \n",
    "            WebDriverWait(driver, 10).until(EC.presence_of_element_located((By.CSS_SELECTOR, \"h1.albumTitle\")))\n",
    "            time.sleep(random.uniform(2, 4))  # more delays\n",
    "\n",
    "            # make soup\n",
    "            album_soup = BeautifulSoup(driver.page_source, \"html.parser\")\n",
    "\n",
    "            # extract album data\n",
    "            album_title = album_soup.select_one(\"h1.albumTitle span[itemprop='name']\")\n",
    "            album_title = album_title.text.strip() if album_title else \"Unknown Album\"\n",
    "\n",
    "            artist_name = album_soup.select_one(\"span[itemprop='name'] a\")\n",
    "            artist_name = artist_name.text.strip() if artist_name else \"Unknown Artist\"\n",
    "\n",
    "            critic_score = album_soup.select_one(\"span[itemprop='ratingValue']\")\n",
    "            critic_score = critic_score.text.strip() if critic_score else \"No Critic Score\"\n",
    "\n",
    "            user_score = album_soup.select_one(\"div.albumUserScore a\")\n",
    "            user_score = user_score.text.strip() if user_score else \"No User Score\"\n",
    "\n",
    "            # write data\n",
    "            writer.writerow([album_title, artist_name, critic_score, user_score])\n",
    "            print(f\"Scraped: {album_title} by {artist_name} | Critic: {critic_score}, User: {user_score}\")\n",
    "\n",
    "            \n",
    "            driver.back()\n",
    "            time.sleep(random.uniform(3, 5))  \n",
    "\n",
    "\n",
    "driver.quit()\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
